{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-05-17T02:37:14.449071Z",
     "iopub.status.busy": "2021-05-17T02:37:14.448560Z",
     "iopub.status.idle": "2021-05-17T02:37:22.101779Z",
     "shell.execute_reply": "2021-05-17T02:37:22.100736Z"
    },
    "papermill": {
     "duration": 7.670677,
     "end_time": "2021-05-17T02:37:22.101976",
     "exception": false,
     "start_time": "2021-05-17T02:37:14.431299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import argparse\n",
    "import albumentations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "#import cudf, cuml, cupy\n",
    "#from cuml.feature_extraction.text import TfidfVectorizer\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors as NearestNeighbors1\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T02:37:22.146863Z",
     "iopub.status.busy": "2021-05-17T02:37:22.136534Z",
     "iopub.status.idle": "2021-05-17T02:37:22.843006Z",
     "shell.execute_reply": "2021-05-17T02:37:22.843458Z"
    },
    "papermill": {
     "duration": 0.730778,
     "end_time": "2021-05-17T02:37:22.843623",
     "exception": false,
     "start_time": "2021-05-17T02:37:22.112845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ShopeeDataset(Dataset):\n",
    "    def __init__(self, df, path, transform=None):\n",
    "        self.df = df\n",
    "        self.path = path\n",
    "        self.file_names = df['image'].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        file_path = self.path + file_name\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        image = image.astype(np.float32)\n",
    "        image = image.transpose(2, 0, 1)\n",
    "\n",
    "        return torch.tensor(image).float()\n",
    "    \n",
    "def get_transforms(image_size):\n",
    "\n",
    "    transforms_train = albumentations.Compose([\n",
    "        albumentations.HorizontalFlip(p=0.5),\n",
    "        albumentations.ImageCompression(quality_lower=99, quality_upper=100),\n",
    "        albumentations.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=10, border_mode=0, p=0.7),\n",
    "        albumentations.Resize(image_size, image_size),\n",
    "        albumentations.Cutout(max_h_size=int(image_size * 0.4), max_w_size=int(image_size * 0.4), num_holes=1, p=0.5),\n",
    "        albumentations.Normalize()\n",
    "    ])\n",
    "\n",
    "    transforms_val = albumentations.Compose([\n",
    "        albumentations.Resize(628, 628),\n",
    "        albumentations.CenterCrop(image_size, image_size, p=1.0),\n",
    "        albumentations.Normalize()\n",
    "    ])\n",
    "\n",
    "    return transforms_train, transforms_val\n",
    "\n",
    "from typing import Dict, Tuple, Any\n",
    "\n",
    "def global_average_precision_score(\n",
    "        y_true: Dict[Any, Any],\n",
    "        y_pred: Dict[Any, Tuple[Any, float]]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute Global Average Precision score (GAP)\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : Dict[Any, Any]\n",
    "        Dictionary with query ids and true ids for query samples\n",
    "    y_pred : Dict[Any, Tuple[Any, float]]\n",
    "        Dictionary with query ids and predictions (predicted id, confidence\n",
    "        level)\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        GAP score\n",
    "    \"\"\"\n",
    "    indexes = list(y_pred.keys())\n",
    "    indexes.sort(\n",
    "        key=lambda x: -y_pred[x][1],\n",
    "    )\n",
    "    queries_with_target = len([i for i in y_true.values() if i is not None])\n",
    "    correct_predictions = 0\n",
    "    total_score = 0.\n",
    "    for i, k in enumerate(indexes, 1):\n",
    "        relevance_of_prediction_i = 0\n",
    "        if y_true[k] == y_pred[k][0]:\n",
    "            correct_predictions += 1\n",
    "            relevance_of_prediction_i = 1\n",
    "        precision_at_rank_i = correct_predictions / i\n",
    "        total_score += precision_at_rank_i * relevance_of_prediction_i\n",
    "\n",
    "    return 1 / queries_with_target * total_score\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append('../input/timm-shopee')\n",
    "import timm\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "class Swish(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i * torch.sigmoid(i)\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        i = ctx.saved_variables[0]\n",
    "        sigmoid_i = torch.sigmoid(i)\n",
    "        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n",
    "\n",
    "class swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return Swish.apply(x)\n",
    "\n",
    "class h_swish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_swish, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.nn.functional.relu6(x + 3., self.inplace) / 6.\n",
    "        return out * x\n",
    "\n",
    "class CrossEntropyLossWithLabelSmoothing(nn.Module):\n",
    "    def __init__(self, n_dim, ls_=0.9):\n",
    "        super().__init__()\n",
    "        self.n_dim = n_dim\n",
    "        self.ls_ = ls_\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        target = F.one_hot(target, self.n_dim).float()\n",
    "        target *= self.ls_\n",
    "        target += (1 - self.ls_) / self.n_dim\n",
    "\n",
    "        logprobs = torch.nn.functional.log_softmax(x, dim=-1)\n",
    "        loss = -logprobs * target\n",
    "        loss = loss.sum(-1)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class DenseCrossEntropy(nn.Module):\n",
    "    def forward(self, x, target):\n",
    "        x = x.float()\n",
    "        target = target.float()\n",
    "        logprobs = torch.nn.functional.log_softmax(x, dim=-1)\n",
    "\n",
    "        loss = -logprobs * target\n",
    "        loss = loss.sum(-1)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class ArcMarginProduct_subcenter(nn.Module):\n",
    "    def __init__(self, in_features, out_features, k=3):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features*k, in_features))\n",
    "        self.reset_parameters()\n",
    "        self.k = k\n",
    "        self.out_features = out_features\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    def forward(self, features):\n",
    "        cosine_all = F.linear(F.normalize(features), F.normalize(self.weight))\n",
    "        cosine_all = cosine_all.view(-1, self.out_features, self.k)\n",
    "        cosine, _ = torch.max(cosine_all, dim=2)\n",
    "        return cosine   \n",
    "\n",
    "\n",
    "class ArcFaceLossAdaptiveMargin(nn.modules.Module):\n",
    "    def __init__(self, margins, s=30.0):\n",
    "        super().__init__()\n",
    "        self.crit = DenseCrossEntropy()\n",
    "        self.s = s\n",
    "        self.margins = margins\n",
    "            \n",
    "    def forward(self, logits, labels, out_dim):\n",
    "        ms = []\n",
    "        ms = self.margins[labels.cpu().numpy()]\n",
    "        cos_m = torch.from_numpy(np.cos(ms)).float().cuda()\n",
    "        sin_m = torch.from_numpy(np.sin(ms)).float().cuda()\n",
    "        th = torch.from_numpy(np.cos(math.pi - ms)).float().cuda()\n",
    "        mm = torch.from_numpy(np.sin(math.pi - ms) * ms).float().cuda()\n",
    "        labels = F.one_hot(labels, out_dim).float()\n",
    "        logits = logits.float()\n",
    "        cosine = logits\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * cos_m.view(-1,1) - sine * sin_m.view(-1,1)\n",
    "        phi = torch.where(cosine > th.view(-1,1), phi, cosine - mm.view(-1,1))\n",
    "        output = (labels * phi) + ((1.0 - labels) * cosine)\n",
    "        output *= self.s\n",
    "        loss = self.crit(output, labels)\n",
    "        return loss\n",
    "\n",
    "def gem(x, p=3, eps=1e-6):\n",
    "    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "\n",
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6, p_trainable=True):\n",
    "        super(GeM,self).__init__()\n",
    "        if p_trainable:\n",
    "            self.p = Parameter(torch.ones(1)*p)\n",
    "        else:\n",
    "            self.p = p\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return gem(x, p=self.p, eps=self.eps)\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T02:37:22.874073Z",
     "iopub.status.busy": "2021-05-17T02:37:22.872410Z",
     "iopub.status.idle": "2021-05-17T02:37:22.874659Z",
     "shell.execute_reply": "2021-05-17T02:37:22.875048Z"
    },
    "papermill": {
     "duration": 0.020649,
     "end_time": "2021-05-17T02:37:22.875162",
     "exception": false,
     "start_time": "2021-05-17T02:37:22.854513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model_Shopee(nn.Module):\n",
    "\n",
    "    def __init__(self, model_name, out_dim):\n",
    "        super(Model_Shopee, self).__init__()\n",
    "\n",
    "        self.net = timm.create_model(model_name, pretrained=False)\n",
    "        self.net.reset_classifier(0, '')\n",
    "        self.embedding_size = 512\n",
    "\n",
    "        self.global_pool = GeM()\n",
    "\n",
    "        self.neck = nn.Sequential(\n",
    "            nn.Linear(self.net.num_features, self.embedding_size, bias=True),\n",
    "            nn.BatchNorm1d(self.embedding_size),\n",
    "            torch.nn.PReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "        self.metric_classify = ArcMarginProduct_subcenter(self.embedding_size, out_dim)\n",
    "\n",
    "    def extract(self, x):\n",
    "        return self.net.forward_features(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.extract(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = x[:, :, 0, 0]\n",
    "        x = self.neck(x)\n",
    "        logits_m = self.metric_classify(x)\n",
    "\n",
    "        return F.normalize(x), logits_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T02:37:22.902742Z",
     "iopub.status.busy": "2021-05-17T02:37:22.902214Z",
     "iopub.status.idle": "2021-05-17T02:37:22.915656Z",
     "shell.execute_reply": "2021-05-17T02:37:22.916097Z"
    },
    "papermill": {
     "duration": 0.030239,
     "end_time": "2021-05-17T02:37:22.916236",
     "exception": false,
     "start_time": "2021-05-17T02:37:22.885997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "ModelClass = Model_Shopee\n",
    "image_size = 560\n",
    "model_name = 'resnet200d'\n",
    "load_from = '../input/checkpoint-test/r200_SD_560_b8_f0_10ep_ftlr4e-6_0.2.pth'\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "\n",
    "GET_CV=False\n",
    "\n",
    "\n",
    "test = pd.read_csv('../input/shopee-product-matching/test.csv')\n",
    "if len(test)>3: GET_CV = False\n",
    "\n",
    "transforms_train, transforms_val = get_transforms(image_size)\n",
    "print(GET_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T02:37:22.947519Z",
     "iopub.status.busy": "2021-05-17T02:37:22.946364Z",
     "iopub.status.idle": "2021-05-17T02:37:22.948740Z",
     "shell.execute_reply": "2021-05-17T02:37:22.949130Z"
    },
    "papermill": {
     "duration": 0.021258,
     "end_time": "2021-05-17T02:37:22.949262",
     "exception": false,
     "start_time": "2021-05-17T02:37:22.928004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getMetric(col):\n",
    "    def f1score(row):\n",
    "        n = len( np.intersect1d(row.target,row[col]) )\n",
    "        return 2*n / (len(row.target)+len(row[col]))\n",
    "    return f1score\n",
    "\n",
    "CHECK_SUB=False\n",
    "def read_dataset():\n",
    "    if GET_CV:\n",
    "        df = pd.read_csv('../input/shopee-product-matching/train.csv')\n",
    "        tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n",
    "        df['matches'] = df['label_group'].map(tmp)\n",
    "        df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n",
    "        if CHECK_SUB:\n",
    "            df = pd.concat([df, df], axis = 0)\n",
    "            df.reset_index(drop = True, inplace = True)\n",
    "        #df_cu = cudf.DataFrame(df)\n",
    "        image_paths = '../input/shopee-product-matching/train_images/'\n",
    "    else:\n",
    "        df = pd.read_csv('../input/shopee-product-matching/test.csv')\n",
    "        #df_cu = cudf.DataFrame(df)\n",
    "        image_paths = '../input/shopee-product-matching/test_images/'\n",
    "        \n",
    "    return df, image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T02:37:22.980614Z",
     "iopub.status.busy": "2021-05-17T02:37:22.978872Z",
     "iopub.status.idle": "2021-05-17T02:37:22.983145Z",
     "shell.execute_reply": "2021-05-17T02:37:22.982393Z"
    },
    "papermill": {
     "duration": 0.022729,
     "end_time": "2021-05-17T02:37:22.983308",
     "exception": false,
     "start_time": "2021-05-17T02:37:22.960579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../input/shopee-product-matching/test_images/\n"
     ]
    }
   ],
   "source": [
    "out_dim=11014\n",
    "test,IMG_PATH=read_dataset()\n",
    "print(IMG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T02:37:23.010963Z",
     "iopub.status.busy": "2021-05-17T02:37:23.010128Z",
     "iopub.status.idle": "2021-05-17T02:37:23.014440Z",
     "shell.execute_reply": "2021-05-17T02:37:23.014037Z"
    },
    "papermill": {
     "duration": 0.018983,
     "end_time": "2021-05-17T02:37:23.014544",
     "exception": false,
     "start_time": "2021-05-17T02:37:22.995561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN dim is  2\n"
     ]
    }
   ],
   "source": [
    "KNN = 50\n",
    "if len(test)==3: KNN = 2\n",
    "print('KNN dim is ',KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T02:37:23.042361Z",
     "iopub.status.busy": "2021-05-17T02:37:23.041566Z",
     "iopub.status.idle": "2021-05-17T02:37:23.058122Z",
     "shell.execute_reply": "2021-05-17T02:37:23.058543Z"
    },
    "papermill": {
     "duration": 0.03198,
     "end_time": "2021-05-17T02:37:23.058665",
     "exception": false,
     "start_time": "2021-05-17T02:37:23.026685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to get our f1 score\n",
    "import gc\n",
    "def f1_score(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    len_y_pred = y_pred.apply(lambda x: len(x)).values\n",
    "    len_y_true = y_true.apply(lambda x: len(x)).values\n",
    "    f1 = 2 * intersection / (len_y_pred + len_y_true)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def get_neighbors(test, embeddings, KNN = 50, image = True):    \n",
    "    if image:\n",
    "        model = NearestNeighbors(n_neighbors = KNN)\n",
    "        model.fit(embeddings)\n",
    "        distances, indices = model.kneighbors(embeddings)\n",
    "    else:\n",
    "        model = NearestNeighbors1(n_neighbors = KNN, metric = 'cosine').fit(embeddings)\n",
    "        distances, indices = model.kneighbors(embeddings)\n",
    "    \n",
    "    # Iterate through different thresholds to maximize cv, run this in interactive mode, then replace else clause with a solid threshold\n",
    "    if GET_CV:\n",
    "        if image:\n",
    "            thresholds = list(np.arange(0.5, 1.1, 0.05))\n",
    "        else:\n",
    "            thresholds = list(np.arange(0.15, 0.6, 0.05))\n",
    "        scores = []\n",
    "        for threshold in thresholds:\n",
    "            predictions = []\n",
    "            for k in range(embeddings.shape[0]):\n",
    "                idx = np.where(distances[k,] < threshold)[0]\n",
    "                ids = indices[k,idx]\n",
    "                posting_ids = ' '.join(test['posting_id'].iloc[ids].values)\n",
    "                predictions.append(posting_ids)\n",
    "            test['pred_matches'] = predictions\n",
    "            test['f1'] = f1_score(test['matches'], test['pred_matches'])\n",
    "            score = test['f1'].mean()\n",
    "            print(f'Our f1 score for threshold {threshold} is {score}')\n",
    "            scores.append(score)\n",
    "        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n",
    "        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n",
    "        best_threshold = max_score['thresholds'].values[0]\n",
    "        best_score = max_score['scores'].values[0]\n",
    "        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n",
    "        \n",
    "        # Use threshold\n",
    "        predictions = []\n",
    "        for k in range(embeddings.shape[0]):\n",
    "            # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n",
    "            if image:\n",
    "                idx = np.where(distances[k,] < 0.95)[0]\n",
    "            else:\n",
    "                idx = np.where(distances[k,] < 0.25)[0]\n",
    "            ids = indices[k,idx]\n",
    "            posting_ids = test['posting_id'].iloc[ids].values\n",
    "            predictions.append(posting_ids)\n",
    "    \n",
    "    # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n",
    "    else:\n",
    "        predictions = []\n",
    "        for k in tqdm(range(embeddings.shape[0])):\n",
    "            if image:\n",
    "                idx = np.where(distances[k,] < 0.95)[0]\n",
    "            else:\n",
    "                idx = np.where(distances[k,] < 0.25)[0]\n",
    "            ids = indices[k,idx]\n",
    "            posting_ids = test['posting_id'].iloc[ids].values\n",
    "            predictions.append(posting_ids)\n",
    "        \n",
    "    del model, distances, indices\n",
    "    gc.collect()\n",
    "    return test, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T02:37:23.222734Z",
     "iopub.status.busy": "2021-05-17T02:37:23.086424Z",
     "iopub.status.idle": "2021-05-17T02:37:23.228653Z",
     "shell.execute_reply": "2021-05-17T02:37:23.229052Z"
    },
    "papermill": {
     "duration": 0.158696,
     "end_time": "2021-05-17T02:37:23.229195",
     "exception": false,
     "start_time": "2021-05-17T02:37:23.070499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing text embeddings...\n",
      "text embeddings shape (3, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\npreds = []\\nCHUNK = 1024*4\\n\\nprint('Finding similar titles...')\\nCTS = len(test)//CHUNK\\nif len(test)%CHUNK!=0: CTS += 1\\nfor j in range( CTS ):\\n    \\n    a = j*CHUNK\\n    b = (j+1)*CHUNK\\n    b = min(b,len(test))\\n    print('chunk',a,'to',b)\\n    \\n    #COSINE SIMILARITY DISTANCE\\n    cts = cupy.matmul(text_embeddings, text_embeddings[a:b].T).T\\n    \\n    for k in range(b-a):\\n        IDX = cupy.where(cts[k,]>0.75)[0]\\n        o = test.iloc[cupy.asnumpy(IDX)].posting_id.values\\n        preds.append(o)\\n        \\ntest['preds_text'] = preds\\ntest.head()\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Computing text embeddings...')\n",
    "model_text = TfidfVectorizer(stop_words=None, \n",
    "                        binary=True, \n",
    "                        max_features=21500)\n",
    "text_embeddings = model_text.fit_transform(test.title)\n",
    "#text_embeddings=text_embeddings.get()\n",
    "print('text embeddings shape',text_embeddings.shape)\n",
    "\n",
    "del model_text\n",
    "gc.collect()\n",
    "\n",
    "'''\n",
    "preds = []\n",
    "CHUNK = 1024*4\n",
    "\n",
    "print('Finding similar titles...')\n",
    "CTS = len(test)//CHUNK\n",
    "if len(test)%CHUNK!=0: CTS += 1\n",
    "for j in range( CTS ):\n",
    "    \n",
    "    a = j*CHUNK\n",
    "    b = (j+1)*CHUNK\n",
    "    b = min(b,len(test))\n",
    "    print('chunk',a,'to',b)\n",
    "    \n",
    "    #COSINE SIMILARITY DISTANCE\n",
    "    cts = cupy.matmul(text_embeddings, text_embeddings[a:b].T).T\n",
    "    \n",
    "    for k in range(b-a):\n",
    "        IDX = cupy.where(cts[k,]>0.75)[0]\n",
    "        o = test.iloc[cupy.asnumpy(IDX)].posting_id.values\n",
    "        preds.append(o)\n",
    "        \n",
    "test['preds_text'] = preds\n",
    "test.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T02:37:23.260326Z",
     "iopub.status.busy": "2021-05-17T02:37:23.259792Z",
     "iopub.status.idle": "2021-05-17T02:37:23.494737Z",
     "shell.execute_reply": "2021-05-17T02:37:23.495129Z"
    },
    "papermill": {
     "duration": 0.2533,
     "end_time": "2021-05-17T02:37:23.495290",
     "exception": false,
     "start_time": "2021-05-17T02:37:23.241990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 4607.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test, predictions = get_neighbors(test, text_embeddings, KNN, image = False)\n",
    "test['preds_text'] = predictions\n",
    "del text_embeddings\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T02:37:23.529943Z",
     "iopub.status.busy": "2021-05-17T02:37:23.528264Z",
     "iopub.status.idle": "2021-05-17T02:37:23.530562Z",
     "shell.execute_reply": "2021-05-17T02:37:23.530952Z"
    },
    "papermill": {
     "duration": 0.021823,
     "end_time": "2021-05-17T02:37:23.531077",
     "exception": false,
     "start_time": "2021-05-17T02:37:23.509254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer = DistilBertTokenizer.from_pretrained('../input/bert-pretrained/distilbert-base-uncased/distilbert-base-uncased/')\n",
    "# dataset_test = ShopeeImageTextDataset(test, IMG_PATH, transform=transforms_val, tokenizer=tokenizer)\n",
    "\n",
    "dataset_test = ShopeeDataset(test, IMG_PATH, transform=transforms_val)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T02:37:23.564781Z",
     "iopub.status.busy": "2021-05-17T02:37:23.564165Z",
     "iopub.status.idle": "2021-05-17T02:37:42.468364Z",
     "shell.execute_reply": "2021-05-17T02:37:42.467377Z"
    },
    "papermill": {
     "duration": 18.923496,
     "end_time": "2021-05-17T02:37:42.468517",
     "exception": false,
     "start_time": "2021-05-17T02:37:23.545021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['SCSE.channel_excitation.0.weight', 'SCSE.channel_excitation.0.bias', 'SCSE.channel_excitation.2.weight', 'SCSE.channel_excitation.2.bias', 'SCSE.spatial_se.0.weight', 'SCSE.spatial_se.0.bias', 'global_pool1.p', 'neck1.0.weight', 'neck1.0.bias', 'neck1.1.weight', 'neck1.1.bias', 'neck1.1.running_mean', 'neck1.1.running_var', 'neck1.1.num_batches_tracked', 'neck1.2.weight'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModelClass(model_name, out_dim=out_dim)\n",
    "model = model.cuda()\n",
    "checkpoint = torch.load(load_from,  map_location='cpu')\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "state_dict = {k[7:] if k.startswith('module.') else k: state_dict[k] for k in state_dict.keys()}    \n",
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T02:37:42.503157Z",
     "iopub.status.busy": "2021-05-17T02:37:42.502625Z",
     "iopub.status.idle": "2021-05-17T02:37:43.768317Z",
     "shell.execute_reply": "2021-05-17T02:37:43.767877Z"
    },
    "papermill": {
     "duration": 1.285059,
     "end_time": "2021-05-17T02:37:43.768444",
     "exception": false,
     "start_time": "2021-05-17T02:37:42.483385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image embeddings shape (3, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader):\n",
    "        data = data.cuda()\n",
    "        feat, _ = model(data)\n",
    "        image_embeddings = feat.detach().cpu().numpy()\n",
    "        embeds.append(image_embeddings)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for (data, input_ids, attention_mask) in tqdm(test_loader):\n",
    "#         data, input_ids, attention_mask = data.cuda(), input_ids.cuda(), attention_mask.cuda()\n",
    "#         feat, _ = model(data, input_ids, attention_mask)\n",
    "#         image_embeddings = feat.detach().cpu().numpy()\n",
    "#         embeds.append(image_embeddings)\n",
    "        \n",
    "image_embeddings = np.concatenate(embeds)\n",
    "print('image embeddings shape',image_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T02:37:43.806053Z",
     "iopub.status.busy": "2021-05-17T02:37:43.805337Z",
     "iopub.status.idle": "2021-05-17T02:37:44.877261Z",
     "shell.execute_reply": "2021-05-17T02:37:44.876804Z"
    },
    "papermill": {
     "duration": 1.092759,
     "end_time": "2021-05-17T02:37:44.877405",
     "exception": false,
     "start_time": "2021-05-17T02:37:43.784646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 2382.68it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test, image_predictions = get_neighbors(test, image_embeddings, KNN, image = True)\n",
    "test['preds_image'] = image_predictions\n",
    "del image_embeddings\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T02:37:44.915998Z",
     "iopub.status.busy": "2021-05-17T02:37:44.915439Z",
     "iopub.status.idle": "2021-05-17T02:37:44.919188Z",
     "shell.execute_reply": "2021-05-17T02:37:44.918764Z"
    },
    "papermill": {
     "duration": 0.023899,
     "end_time": "2021-05-17T02:37:44.919347",
     "exception": false,
     "start_time": "2021-05-17T02:37:44.895448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tmp = test.groupby('image_phash').posting_id.agg('unique').to_dict()\n",
    "# test['preds_phash'] = test.image_phash.map(tmp)\n",
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T02:37:44.961298Z",
     "iopub.status.busy": "2021-05-17T02:37:44.960561Z",
     "iopub.status.idle": "2021-05-17T02:37:44.963168Z",
     "shell.execute_reply": "2021-05-17T02:37:44.963702Z"
    },
    "papermill": {
     "duration": 0.026329,
     "end_time": "2021-05-17T02:37:44.963830",
     "exception": false,
     "start_time": "2021-05-17T02:37:44.937501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_for_sub(row):\n",
    "    x = np.concatenate([row.preds_image,row.preds_text])\n",
    "    return ' '.join(np.unique(x))\n",
    "\n",
    "def combine_for_cv(row):\n",
    "    x = np.concatenate([row.preds_image,row.preds_text])\n",
    "    return np.unique(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T02:37:45.004929Z",
     "iopub.status.busy": "2021-05-17T02:37:45.004196Z",
     "iopub.status.idle": "2021-05-17T02:37:45.007085Z",
     "shell.execute_reply": "2021-05-17T02:37:45.006626Z"
    },
    "papermill": {
     "duration": 0.026183,
     "end_time": "2021-05-17T02:37:45.007185",
     "exception": false,
     "start_time": "2021-05-17T02:37:44.981002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if GET_CV:\n",
    "    tmp = test.groupby('label_group').posting_id.agg('unique').to_dict()\n",
    "    test['target'] = test.label_group.map(tmp)\n",
    "    test['oof'] = test.apply(combine_for_cv,axis=1)\n",
    "    test['f1'] = test.apply(getMetric('oof'),axis=1)\n",
    "    print('CV Score =', test.f1.mean() )\n",
    "    print('===========================================\\n')\n",
    "    \n",
    "    print(\"CV for image :\", round(test.apply(getMetric('preds_image'),axis=1).mean(), 3))\n",
    "    print(\"CV for text  :\", round(test.apply(getMetric('preds_text'),axis=1).mean(), 3))\n",
    "#     print(\"CV for phash :\", round(test.apply(getMetric('preds_phash'),axis=1).mean(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T02:37:45.046339Z",
     "iopub.status.busy": "2021-05-17T02:37:45.045749Z",
     "iopub.status.idle": "2021-05-17T02:37:45.199308Z",
     "shell.execute_reply": "2021-05-17T02:37:45.199734Z"
    },
    "papermill": {
     "duration": 0.175367,
     "end_time": "2021-05-17T02:37:45.199876",
     "exception": false,
     "start_time": "2021-05-17T02:37:45.024509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_2255846744</td>\n",
       "      <td>test_2255846744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_3588702337</td>\n",
       "      <td>test_3588702337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_4015706929</td>\n",
       "      <td>test_4015706929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        posting_id          matches\n",
       "0  test_2255846744  test_2255846744\n",
       "1  test_3588702337  test_3588702337\n",
       "2  test_4015706929  test_4015706929"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['matches'] = test.apply(combine_for_sub,axis=1)\n",
    "test[['posting_id','matches']].to_csv('submission.csv',index=False)\n",
    "sub = pd.read_csv('submission.csv')\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.019551,
     "end_time": "2021-05-17T02:37:45.238526",
     "exception": false,
     "start_time": "2021-05-17T02:37:45.218975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 37.394782,
   "end_time": "2021-05-17T02:37:46.770702",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-17T02:37:09.375920",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
